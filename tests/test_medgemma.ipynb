{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedGemma Inference Test\n",
    "\n",
    "This notebook tests the MedGemma client with **Ollama backend**.\n",
    "\n",
    "MedGemma is a collection of **Gemma 3** variants trained for medical text and image comprehension.\n",
    "The 4B variant is **multimodal** — it can process both text and images.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Install Ollama: https://ollama.ai\n",
    "2. Pull the model: `ollama pull MedAIBase/MedGemma1.5:4b-it-q8_0`\n",
    "3. Ollama server running (starts automatically or run `ollama serve`)\n",
    "\n",
    "## Tests\n",
    "- **Sections 1-6**: Text-only inference (reference ranges, clinical interpretation, JSON extraction)\n",
    "- **Sections 7-10**: **Image-based inference** (multimodal — prescription, lab report, PDF pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/yaokouadio/Projects/STARTUP/universal-medical-ingestion-engine\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging configured\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "print(\"Logging configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Client (Ollama Backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yaokouadio/Projects/STARTUP/universal-medical-ingestion-engine/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Ollama config:\n",
      "  backend: ollama\n",
      "  max_tokens: 1000\n",
      "  temperature: 0.1\n",
      "  use_cache: True\n",
      "  ollama_host: http://localhost:11434\n",
      "  ollama_model: MedAIBase/MedGemma1.5:4b-it-q8_0\n",
      "  timeout: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yaokouadio/Projects/STARTUP/universal-medical-ingestion-engine/.venv/lib/python3.11/site-packages/pypdf/_crypt_providers/_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
     ]
    }
   ],
   "source": [
    "from src.medical_ingestion.medgemma.client import create_client, get_default_config\n",
    "\n",
    "# Show default config\n",
    "default_config = get_default_config('ollama')\n",
    "print(\"Default Ollama config:\")\n",
    "for k, v in default_config.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 19:53:20,119 - OllamaMedGemmaClient - INFO - Initialized Ollama client: http://localhost:11434 / medgemma-4b-local\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client created\n",
      "  Backend: ollama\n",
      "  Model: medgemma-4b-local\n"
     ]
    }
   ],
   "source": [
    "# Create client with Ollama backend\n",
    "config = {\n",
    "    'backend': 'ollama',\n",
    "    'ollama_host': 'http://localhost:11434',\n",
    "    'ollama_model': 'medgemma-4b-local',\n",
    "    'max_tokens': 512,\n",
    "    'temperature': 0.1,\n",
    "}\n",
    "\n",
    "client = create_client(config)\n",
    "\n",
    "print(f\"Client created\")\n",
    "print(f\"  Backend: {client.backend_type.value}\")\n",
    "print(f\"  Model: {client.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health Check:\n",
      "  Healthy: True\n",
      "  Backend: ollama\n",
      "  Model: medgemma-4b-local\n",
      "  Details: Ollama server running and model available\n"
     ]
    }
   ],
   "source": [
    "# Check if Ollama is running and model is available\n",
    "health = await client.health_check()\n",
    "\n",
    "print(\"Health Check:\")\n",
    "print(f\"  Healthy: {health['healthy']}\")\n",
    "print(f\"  Backend: {health['backend']}\")\n",
    "print(f\"  Model: {health['model']}\")\n",
    "print(f\"  Details: {health['details']}\")\n",
    "\n",
    "if not health['healthy']:\n",
    "    print(\"\\n*** FIX REQUIRED ***\")\n",
    "    print(\"Run these commands:\")\n",
    "    print(\"  1. ollama serve  (if not running)\")\n",
    "    print(\"  2. ollama pull MedAIBase/MedGemma1.5:4b-it-q8_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Inference - Simple Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "What are the normal reference ranges for the following lab values?\n",
      "- Hemoglobin\n",
      "- White Blood Cell Count\n",
      "- Platelet Count\n",
      "\n",
      "Provide a brief answer.\n",
      "\n",
      "============================================================\n",
      "Generating response...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 19:53:40,990 - OllamaMedGemmaClient - INFO - Generated 185 tokens in 8.65s (31.1 tokens/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESPONSE:\n",
      "============================================================\n",
      "Here are the normal reference ranges for the lab values you requested:\n",
      "\n",
      "*   **Hemoglobin:** 12.0 - 16.0 g/dL (men), 12.0 - 16.0 g/dL (women)\n",
      "*   **White Blood Cell Count:** 4.5 - 11.0 x 10^9/L (4.5 - 11.0 x 10^3/mm^3)\n",
      "*   **Platelet Count:** 150 - 400 x 10^9/L (150 - 400 x 10^3/mm^3)\n",
      "\n",
      "**Note:** These ranges can vary slightly depending on the laboratory and the specific testing method used. Always refer to the reference ranges provided by the specific lab that performed the test.\n",
      "============================================================\n",
      "\n",
      "Prompt tokens: 43\n",
      "Generated tokens: 185\n",
      "Inference time: 8.65s\n",
      "Tokens/sec: 31.1\n"
     ]
    }
   ],
   "source": [
    "# Simple medical query\n",
    "prompt = \"\"\"What are the normal reference ranges for the following lab values?\n",
    "- Hemoglobin\n",
    "- White Blood Cell Count\n",
    "- Platelet Count\n",
    "\n",
    "Provide a brief answer.\"\"\"\n",
    "\n",
    "print(\"Prompt:\")\n",
    "print(prompt)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Generating response...\\n\")\n",
    "\n",
    "result = await client.generate(prompt, max_tokens=256)\n",
    "\n",
    "print(\"RESPONSE:\")\n",
    "print(\"=\"*60)\n",
    "print(result['text'])\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPrompt tokens: {result['prompt_tokens']}\")\n",
    "print(f\"Generated tokens: {result['generated_tokens']}\")\n",
    "print(f\"Inference time: {result['inference_time']:.2f}s\")\n",
    "print(f\"Tokens/sec: {result.get('tokens_per_second', 0):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Inference - Lab Result Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating clinical interpretation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 19:56:55,538 - OllamaMedGemmaClient - INFO - Generated 136 tokens in 6.65s (31.1 tokens/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLINICAL INTERPRETATION:\n",
      "============================================================\n",
      "Overall Assessment:\n",
      "- The patient has diabetes mellitus, hypercholesterolemia, high LDL, low HDL, and hypertriglyceridemia.\n",
      "\n",
      "Detailed Interpretation:\n",
      "- Glucose (fasting): 142 mg/dL — Diabetes Mellitus\n",
      "- HbA1c: 7.2% — Diabetes Mellitus\n",
      "- Total Cholesterol: 245 mg/dL — Hypercholesterolemia\n",
      "- LDL: 165 mg/dL — High LDL\n",
      "- HDL: 38 mg/dL — Low HDL\n",
      "- Triglycerides: 210 mg/dL — Hypertriglyceridemia\n",
      "============================================================\n",
      "\n",
      "Inference time: 6.65s\n",
      "Tokens/sec: 31.1\n"
     ]
    }
   ],
   "source": [
    "# Lab result interpretation\n",
    "prompt = \"\"\"You are a medical assistant AI designed to provide clinical interpretations of lab results.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "\n",
    "1. Apply **diagnostic thresholds exactly**. Do NOT downgrade, hedge, or mislabel a diagnosis if a lab value meets established criteria.\n",
    "\n",
    "   - Fasting glucose ≥126 mg/dL = Diabetes Mellitus\n",
    "   - HbA1c ≥6.5% = Diabetes Mellitus\n",
    "   - Total cholesterol ≥200 mg/dL = Hypercholesterolemia\n",
    "   - LDL ≥130 mg/dL = High LDL\n",
    "   - HDL <40 mg/dL (male) = Low HDL\n",
    "   - Triglycerides ≥150 mg/dL = Hypertriglyceridemia\n",
    "\n",
    "2. First, **determine the relevant diagnoses** based on the labs. Only include diagnoses that are supported by the numbers above.\n",
    "\n",
    "3. Then, **explain the clinical significance** of these lab abnormalities in clear, concise language, suitable for a clinician.\n",
    "\n",
    "4. Suggest **general next steps or risk implications**, but do NOT suggest treatments or medications unless explicitly asked.\n",
    "\n",
    "5. **Output format must be structured as follows**:\n",
    "\n",
    "Overall Assessment:\n",
    "- <Brief summary of major diagnoses and risks>\n",
    "\n",
    "Detailed Interpretation:\n",
    "- <Lab Name>: <Value> — <Interpretation including normal range and significance>\n",
    "- ...\n",
    "\n",
    "Important: Never hedge. If thresholds are met, explicitly state the diagnosis.\n",
    "\n",
    "---\n",
    "\n",
    "Now analyze the following lab results for a 45-year-old male patient:\n",
    "\n",
    "- Glucose (fasting): 142 mg/dL\n",
    "- HbA1c: 7.2%\n",
    "- Total Cholesterol: 245 mg/dL\n",
    "- LDL: 165 mg/dL\n",
    "- HDL: 38 mg/dL\n",
    "- Triglycerides: 210 mg/dL\n",
    "\"\"\"\n",
    "\n",
    "print(\"Generating clinical interpretation...\\n\")\n",
    "\n",
    "result = await client.generate(prompt, max_tokens=512)\n",
    "\n",
    "print(\"CLINICAL INTERPRETATION:\")\n",
    "print(\"=\"*60)\n",
    "print(result['text'])\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nInference time: {result['inference_time']:.2f}s\")\n",
    "print(f\"Tokens/sec: {result.get('tokens_per_second', 0):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test JSON Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating structured response...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 20:05:56,861 - OllamaMedGemmaClient - INFO - Generated 176 tokens in 9.59s (31.8 tokens/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response:\n",
      "```json\n",
      "{\n",
      "  \"patient\": \"John Smith\",\n",
      "  \"date\": \"2024-01-15\",\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"test\": \"Hemoglobin\",\n",
      "      \"value\": 14.2,\n",
      "      \"unit\": \"g/dL\",\n",
      "      \"normal\": true\n",
      "    },\n",
      "    {\n",
      "      \"test\": \"WBC\",\n",
      "      \"value\": 8.5,\n",
      "      \"unit\": \"x10^9/L\",\n",
      "      \"normal\": true\n",
      "    },\n",
      "    {\n",
      "      \"test\": \"Platelets\",\n",
      "      \"value\": 250,\n",
      "      \"unit\": \"x10^9/L\",\n",
      "      \"normal\": true\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "============================================================\n",
      "\n",
      "Extracted JSON:\n",
      "{\n",
      "  \"patient\": \"John Smith\",\n",
      "  \"date\": \"2024-01-15\",\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"test\": \"Hemoglobin\",\n",
      "      \"value\": 14.2,\n",
      "      \"unit\": \"g/dL\",\n",
      "      \"normal\": true\n",
      "    },\n",
      "    {\n",
      "      \"test\": \"WBC\",\n",
      "      \"value\": 8.5,\n",
      "      \"unit\": \"x10^9/L\",\n",
      "      \"normal\": true\n",
      "    },\n",
      "    {\n",
      "      \"test\": \"Platelets\",\n",
      "      \"value\": 250,\n",
      "      \"unit\": \"x10^9/L\",\n",
      "      \"normal\": true\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test structured output\n",
    "prompt = \"\"\"You are a medical data extraction AI. Your task is to extract lab values from the following report and return **only valid JSON**, strictly matching this structure:\n",
    "\n",
    "{\n",
    "  \"patient\": \"name\",\n",
    "  \"date\": \"YYYY-MM-DD\",\n",
    "  \"results\": [\n",
    "    {\n",
    "      \"test\": \"name\",\n",
    "      \"value\": number,\n",
    "      \"unit\": \"unit\",\n",
    "      \"normal\": true/false\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "Rules:\n",
    "1. Do NOT include explanations, reasoning, or extra text.\n",
    "2. All string values must be in double quotes.\n",
    "3. Boolean values must be `true` or `false`.\n",
    "4. Output must be **strict JSON parsable**.\n",
    "5. Normal flag is true if the value falls within the given normal range.\n",
    "\n",
    "Here is the report:\n",
    "\n",
    "Patient: John Smith\n",
    "Test Date: 2024-01-15\n",
    "Hemoglobin: 14.2 g/dL (Normal: 13.5-17.5)\n",
    "WBC: 8.5 x10^9/L (Normal: 4.5-11.0)\n",
    "Platelets: 250 x10^9/L (Normal: 150-400)\n",
    "\n",
    "Return the JSON now:\n",
    "\"\"\"\n",
    "\n",
    "print(\"Generating structured response...\\n\")\n",
    "\n",
    "result = await client.generate(prompt, max_tokens=512, temperature=0.0)\n",
    "\n",
    "print(\"Raw response:\")\n",
    "print(result['text'])\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Extract JSON\n",
    "extracted = client.extract_json(result['text'])\n",
    "if extracted:\n",
    "    print(\"\\nExtracted JSON:\")\n",
    "    import json\n",
    "    print(json.dumps(extracted, indent=2))\n",
    "else:\n",
    "    print(\"\\nCould not extract JSON from response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. View Statistics (Text-Only Tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedGemma Client Statistics\n",
      "========================================\n",
      "backend: ollama\n",
      "model: medgemma-4b-local\n",
      "inference_count: 3\n",
      "cache_hits: 0\n",
      "cache_hit_rate: 0.00\n",
      "total_inference_time: 24.89\n",
      "average_inference_time: 8.30\n",
      "ollama_host: http://localhost:11434\n"
     ]
    }
   ],
   "source": [
    "stats = client.get_statistics()\n",
    "\n",
    "print(\"MedGemma Client Statistics\")\n",
    "print(\"=\"*40)\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Different Backends\n",
    "\n",
    "You can switch backends by changing the config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available backends:\n",
      "\n",
      "1. OLLAMA (current - recommended):\n",
      "   config = {'backend': 'ollama'}\n",
      "   - Easy setup, efficient quantization\n",
      "   - Requires: ollama serve + ollama pull <model>\n",
      "\n",
      "2. LOCAL (HuggingFace Transformers):\n",
      "   config = {'backend': 'local', 'model_path': './models/cache/medgemma'}\n",
      "   - Full control, works offline\n",
      "   - Requires: 16GB+ RAM, GPU recommended\n",
      "\n",
      "3. API (not yet implemented):\n",
      "   config = {'backend': 'api'}\n",
      "   - Cloud-based inference\n"
     ]
    }
   ],
   "source": [
    "# Example: How to use different backends\n",
    "\n",
    "print(\"Available backends:\")\n",
    "print()\n",
    "print(\"1. OLLAMA (current - recommended):\")\n",
    "print(\"   config = {'backend': 'ollama'}\")\n",
    "print(\"   - Easy setup, efficient quantization\")\n",
    "print(\"   - Requires: ollama serve + ollama pull <model>\")\n",
    "print()\n",
    "print(\"2. LOCAL (HuggingFace Transformers):\")\n",
    "print(\"   config = {'backend': 'local', 'model_path': './models/cache/medgemma'}\")\n",
    "print(\"   - Full control, works offline\")\n",
    "print(\"   - Requires: 16GB+ RAM, GPU recommended\")\n",
    "print()\n",
    "print(\"3. API (not yet implemented):\")\n",
    "print(\"   config = {'backend': 'api'}\")\n",
    "print(\"   - Cloud-based inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Helper: Send Image to Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_with_image() helper ready\n",
      "  Model: medgemma-4b-local\n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import base64\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "OLLAMA_MODEL = \"medgemma-4b-local\"\n",
    "\n",
    "async def generate_with_image(\n",
    "    prompt: str,\n",
    "    image_path: str = None,\n",
    "    image_bytes: bytes = None,\n",
    "    model: str = OLLAMA_MODEL,\n",
    "    max_tokens: int = 1024,\n",
    "    temperature: float = 0.1,\n",
    "    timeout_sec: int = 300\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Call Ollama /api/generate with an image (multimodal).\n",
    "    \n",
    "    Ollama accepts images as base64 strings in the 'images' array.\n",
    "    Works with MedGemma 4B multimodal, llava, minicpm-v, etc.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Text prompt\n",
    "        image_path: Path to image file (PNG, JPG, etc.)\n",
    "        image_bytes: Raw image bytes (alternative to image_path)\n",
    "        model: Ollama model name\n",
    "        max_tokens: Max tokens to generate\n",
    "        temperature: Sampling temperature\n",
    "        timeout_sec: Request timeout\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'text', 'inference_time', 'prompt_tokens', 'generated_tokens'\n",
    "    \"\"\"\n",
    "    # Encode image to base64\n",
    "    if image_path:\n",
    "        with open(image_path, 'rb') as f:\n",
    "            img_b64 = base64.b64encode(f.read()).decode('utf-8')\n",
    "        print(f\"  Image: {Path(image_path).name} ({Path(image_path).stat().st_size / 1024:.0f} KB)\")\n",
    "    elif image_bytes:\n",
    "        img_b64 = base64.b64encode(image_bytes).decode('utf-8')\n",
    "        print(f\"  Image: {len(image_bytes) / 1024:.0f} KB (from bytes)\")\n",
    "    else:\n",
    "        raise ValueError(\"Provide image_path or image_bytes\")\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"images\": [img_b64],\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"num_predict\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    start = time.time()\n",
    "    timeout = aiohttp.ClientTimeout(total=timeout_sec)\n",
    "    \n",
    "    async with aiohttp.ClientSession(timeout=timeout) as session:\n",
    "        async with session.post(f\"{OLLAMA_HOST}/api/generate\", json=payload) as resp:\n",
    "            if resp.status != 200:\n",
    "                error = await resp.text()\n",
    "                raise RuntimeError(f\"Ollama error ({resp.status}): {error}\")\n",
    "            data = await resp.json()\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    text = data.get('response', '')\n",
    "    prompt_tokens = data.get('prompt_eval_count', 0)\n",
    "    gen_tokens = data.get('eval_count', 0)\n",
    "    eval_ns = data.get('eval_duration', 0)\n",
    "    tps = gen_tokens / (eval_ns / 1e9) if eval_ns > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'text': text.strip(),\n",
    "        'inference_time': elapsed,\n",
    "        'prompt_tokens': prompt_tokens,\n",
    "        'generated_tokens': gen_tokens,\n",
    "        'tokens_per_second': tps,\n",
    "    }\n",
    "\n",
    "print(\"generate_with_image() helper ready\")\n",
    "print(f\"  Model: {OLLAMA_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Image Test — Prescription (PNG)\n",
    "\n",
    "Test MedGemma's ability to read a prescription image and extract medication info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST: Prescription Image -> Describe contents\n",
      "============================================================\n",
      "  Image: ColorRx-English-Logo-HTWT-Generics.png (129 KB)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Ollama error (500): {\"error\":\"Failed to create new sequence: failed to process inputs: this model is missing data required for image input\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m      8\u001b[39m prompt = \u001b[33m\"\"\"\u001b[39m\u001b[33mLook at this medical document image. Describe what you see:\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[33m1. What type of document is this?\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[33m2. What information can you read from it?\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[33m3. List any medications, dosages, or patient info visible.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m generate_with_image(prompt, image_path=rx_image, max_tokens=\u001b[32m512\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRESPONSE (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33minference_time\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mtokens_per_second\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tok/s):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mgenerate_with_image\u001b[39m\u001b[34m(prompt, image_path, image_bytes, model, max_tokens, temperature, timeout_sec)\u001b[39m\n\u001b[32m     64\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m resp.status != \u001b[32m200\u001b[39m:\n\u001b[32m     65\u001b[39m             error = \u001b[38;5;28;01mawait\u001b[39;00m resp.text()\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOllama error (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresp.status\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     67\u001b[39m         data = \u001b[38;5;28;01mawait\u001b[39;00m resp.json()\n\u001b[32m     69\u001b[39m elapsed = time.time() - start\n",
      "\u001b[31mRuntimeError\u001b[39m: Ollama error (500): {\"error\":\"Failed to create new sequence: failed to process inputs: this model is missing data required for image input\"}"
     ]
    }
   ],
   "source": [
    "# Test with a prescription image\n",
    "rx_image = os.path.join(project_root, \"data/samples/prescriptions/ColorRx-English-Logo-HTWT-Generics.png\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST: Prescription Image -> Describe contents\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prompt = \"\"\"Look at this medical document image. Describe what you see:\n",
    "1. What type of document is this?\n",
    "2. What information can you read from it?\n",
    "3. List any medications, dosages, or patient info visible.\"\"\"\n",
    "\n",
    "result = await generate_with_image(prompt, image_path=rx_image, max_tokens=512)\n",
    "\n",
    "print(f\"\\nRESPONSE ({result['inference_time']:.1f}s, {result['tokens_per_second']:.1f} tok/s):\")\n",
    "print(\"-\" * 60)\n",
    "print(result['text'])\n",
    "print(\"-\" * 60)\n",
    "print(f\"Prompt tokens: {result['prompt_tokens']}, Generated: {result['generated_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Image Test — Lab Report (PNG)\n",
    "\n",
    "Test MedGemma's ability to read a LabCorp result image and extract test values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a lab report image (LabCorp positive result)\n",
    "lab_image = os.path.join(project_root, \"data/samples/labs/labcorp/labcorp-positive.png\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST: Lab Report Image -> Extract test results as JSON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prompt = \"\"\"Extract ALL test results from this lab report image as JSON.\n",
    "\n",
    "Return JSON with this structure:\n",
    "{\"test_results\": [{\"name\": \"test name\", \"value\": \"result\", \"unit\": \"unit\", \"reference_range\": \"range\", \"abnormal_flag\": \"H/L/null\"}]}\n",
    "\n",
    "Rules:\n",
    "- Extract EVERY test result visible in the image\n",
    "- Include the exact values, units, and reference ranges shown\n",
    "- Mark abnormal values with H (high) or L (low)\n",
    "- Return ONLY valid JSON, no explanations\"\"\"\n",
    "\n",
    "result = await generate_with_image(prompt, image_path=lab_image, max_tokens=1024)\n",
    "\n",
    "print(f\"\\nRESPONSE ({result['inference_time']:.1f}s, {result['tokens_per_second']:.1f} tok/s):\")\n",
    "print(\"-\" * 60)\n",
    "print(result['text'][:2000])  # Truncate if very long\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Try to parse JSON from response\n",
    "from json_repair import repair_json\n",
    "import re\n",
    "\n",
    "raw = result['text']\n",
    "json_match = re.search(r'(\\{[\\s\\S]*\\})', raw)\n",
    "if json_match:\n",
    "    try:\n",
    "        parsed = json.loads(json_match.group(1))\n",
    "        tests = parsed.get('test_results', [])\n",
    "        print(f\"\\nParsed {len(tests)} test results:\")\n",
    "        for t in tests:\n",
    "            flag = f\" [{t.get('abnormal_flag')}]\" if t.get('abnormal_flag') else \"\"\n",
    "            print(f\"  {t.get('name')}: {t.get('value')} {t.get('unit', '')}{flag}\")\n",
    "    except json.JSONDecodeError:\n",
    "        try:\n",
    "            repaired = repair_json(json_match.group(1))\n",
    "            parsed = json.loads(repaired)\n",
    "            tests = parsed.get('test_results', [])\n",
    "            print(f\"\\nParsed {len(tests)} test results (after repair):\")\n",
    "            for t in tests:\n",
    "                flag = f\" [{t.get('abnormal_flag')}]\" if t.get('abnormal_flag') else \"\"\n",
    "                print(f\"  {t.get('name')}: {t.get('value')} {t.get('unit', '')}{flag}\")\n",
    "        except:\n",
    "            print(\"\\nCould not parse JSON from response\")\n",
    "else:\n",
    "    print(\"\\nNo JSON found in response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Image Test — PDF Page (LabCorp CBC)\n",
    "\n",
    "Convert the first page of a PDF to an image, then send to MedGemma.\n",
    "This tests the full vision pipeline: PDF -> image -> multimodal extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PDF first page to image, then send to MedGemma\n",
    "import pymupdf\n",
    "\n",
    "pdf_path = os.path.join(project_root, \"data/samples/labs/labcorp/SampleLabCorpReport.pdf\")\n",
    "\n",
    "# Convert first page to PNG\n",
    "doc = pymupdf.open(pdf_path)\n",
    "page = doc[0]\n",
    "mat = pymupdf.Matrix(150/72, 150/72)  # 150 DPI\n",
    "pix = page.get_pixmap(matrix=mat)\n",
    "page_bytes = pix.tobytes(\"png\")\n",
    "doc.close()\n",
    "\n",
    "print(f\"PDF: {Path(pdf_path).name}\")\n",
    "print(f\"Page 1 rendered: {pix.width}x{pix.height} px, {len(page_bytes)/1024:.0f} KB\")\n",
    "print()\n",
    "\n",
    "# First test: describe the document\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST A: PDF Page -> Describe document type\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prompt_describe = \"\"\"What type of medical document is this? \n",
    "Describe what you see: the layout, sections, and type of data present.\"\"\"\n",
    "\n",
    "result = await generate_with_image(prompt_describe, image_bytes=page_bytes, max_tokens=256)\n",
    "\n",
    "print(f\"\\nRESPONSE ({result['inference_time']:.1f}s):\")\n",
    "print(\"-\" * 60)\n",
    "print(result['text'])\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second test: structured extraction from same PDF page\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST B: PDF Page -> Extract ALL test results as JSON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prompt_extract = \"\"\"Extract ALL test results from this lab report image as JSON.\n",
    "\n",
    "Return JSON:\n",
    "{\"patient\": {\"name\": \"full name\", \"dob\": \"date of birth\"},\n",
    " \"test_results\": [{\"name\": \"test name\", \"value\": \"result\", \"unit\": \"unit\", \"reference_range\": \"range\", \"abnormal_flag\": \"H/L/null\"}]}\n",
    "\n",
    "CRITICAL: Extract EVERY single test result row visible. Include CBC, differentials, chemistry, everything.\n",
    "Return ONLY valid JSON:\"\"\"\n",
    "\n",
    "result = await generate_with_image(prompt_extract, image_bytes=page_bytes, max_tokens=2048)\n",
    "\n",
    "print(f\"\\nRESPONSE ({result['inference_time']:.1f}s, {result['tokens_per_second']:.1f} tok/s):\")\n",
    "print(\"-\" * 60)\n",
    "print(result['text'][:3000])\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Parse and summarize\n",
    "raw = result['text']\n",
    "json_match = re.search(r'(\\{[\\s\\S]*\\})', raw)\n",
    "if json_match:\n",
    "    try:\n",
    "        parsed = json.loads(json_match.group(1))\n",
    "    except json.JSONDecodeError:\n",
    "        try:\n",
    "            repaired = repair_json(json_match.group(1))\n",
    "            parsed = json.loads(repaired)\n",
    "        except:\n",
    "            parsed = None\n",
    "    \n",
    "    if parsed:\n",
    "        patient = parsed.get('patient', {})\n",
    "        tests = parsed.get('test_results', [])\n",
    "        print(f\"\\nPatient: {patient.get('name', 'N/A')}, DOB: {patient.get('dob', 'N/A')}\")\n",
    "        print(f\"Total test results extracted: {len(tests)}\")\n",
    "        print()\n",
    "        \n",
    "        # Show all tests\n",
    "        for t in tests:\n",
    "            flag = f\" [{t.get('abnormal_flag')}]\" if t.get('abnormal_flag') else \"\"\n",
    "            ref = f\" (ref: {t.get('reference_range')})\" if t.get('reference_range') else \"\"\n",
    "            print(f\"  {t.get('name')}: {t.get('value')} {t.get('unit', '')}{flag}{ref}\")\n",
    "        \n",
    "        # Check for key CBC tests\n",
    "        test_names_lower = [t.get('name', '').lower() for t in tests]\n",
    "        cbc_expected = ['wbc', 'rbc', 'hemoglobin', 'hematocrit', 'platelet', 'neutrophil', 'lymphocyte', 'monocyte', 'eosinophil', 'basophil']\n",
    "        found = [name for name in cbc_expected if any(name in tn for tn in test_names_lower)]\n",
    "        missing = [name for name in cbc_expected if not any(name in tn for tn in test_names_lower)]\n",
    "        print(f\"\\nCBC coverage: {len(found)}/{len(cbc_expected)}\")\n",
    "        if missing:\n",
    "            print(f\"  Missing: {missing}\")\n",
    "    else:\n",
    "        print(\"\\nCould not parse JSON\")\n",
    "else:\n",
    "    print(\"\\nNo JSON found in response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Image Test — Handwritten Prescription (JPG)\n",
    "\n",
    "The hardest test: can MedGemma read a handwritten prescription?\n",
    "This is where multimodal models shine vs traditional OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with handwritten prescription\n",
    "hw_image = os.path.join(project_root, \"data/samples/prescriptions/Hassprescription-768x576.jpg\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST: Handwritten Prescription -> Read and extract\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prompt = \"\"\"This is a handwritten medical prescription. Read it carefully and extract:\n",
    "\n",
    "1. Patient name (if visible)\n",
    "2. Doctor/prescriber name (if visible)\n",
    "3. Each medication with dosage and instructions\n",
    "4. Date (if visible)\n",
    "\n",
    "Be specific about what you can and cannot read. If text is illegible, say so.\"\"\"\n",
    "\n",
    "result = await generate_with_image(prompt, image_path=hw_image, max_tokens=512)\n",
    "\n",
    "print(f\"\\nRESPONSE ({result['inference_time']:.1f}s, {result['tokens_per_second']:.1f} tok/s):\")\n",
    "print(\"-\" * 60)\n",
    "print(result['text'])\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Image Test — Multi-Page PDF (All Pages)\n",
    "\n",
    "Process each page of a multi-page PDF separately and merge results.\n",
    "This simulates what the pipeline would do with chunked vision extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process ALL pages of a multi-page lab report PDF\n",
    "pdf_path = os.path.join(project_root, \"data/samples/labs/labcorp/SampleLabCorpReport.pdf\")\n",
    "\n",
    "doc = pymupdf.open(pdf_path)\n",
    "print(f\"PDF: {Path(pdf_path).name} — {len(doc)} pages\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_tests = []\n",
    "total_time = 0\n",
    "\n",
    "prompt_page = \"\"\"Extract ALL test results from this lab report page as JSON.\n",
    "\n",
    "Return JSON:\n",
    "{\"test_results\": [{\"name\": \"test name\", \"value\": \"result\", \"unit\": \"unit\", \"reference_range\": \"range\", \"abnormal_flag\": \"H/L/null\"}]}\n",
    "\n",
    "Extract EVERY row with a test name and value. Return [] if no test results on this page.\n",
    "Return ONLY valid JSON:\"\"\"\n",
    "\n",
    "for page_num in range(len(doc)):\n",
    "    page = doc[page_num]\n",
    "    mat = pymupdf.Matrix(150/72, 150/72)\n",
    "    pix = page.get_pixmap(matrix=mat)\n",
    "    pg_bytes = pix.tobytes(\"png\")\n",
    "    \n",
    "    print(f\"\\nPage {page_num + 1}/{len(doc)} ({len(pg_bytes)/1024:.0f} KB)...\")\n",
    "    \n",
    "    try:\n",
    "        result = await generate_with_image(prompt_page, image_bytes=pg_bytes, max_tokens=2048)\n",
    "        total_time += result['inference_time']\n",
    "        \n",
    "        raw = result['text']\n",
    "        json_match = re.search(r'(\\{[\\s\\S]*\\})', raw)\n",
    "        if json_match:\n",
    "            try:\n",
    "                parsed = json.loads(json_match.group(1))\n",
    "            except:\n",
    "                try:\n",
    "                    repaired = repair_json(json_match.group(1))\n",
    "                    parsed = json.loads(repaired)\n",
    "                except:\n",
    "                    parsed = None\n",
    "            \n",
    "            if parsed:\n",
    "                page_tests = parsed.get('test_results', [])\n",
    "                # Filter tests with actual values\n",
    "                page_tests = [t for t in page_tests if t.get('value') and str(t['value']).strip() not in ('', 'null', 'None')]\n",
    "                all_tests.extend(page_tests)\n",
    "                print(f\"  -> {len(page_tests)} test results ({result['inference_time']:.1f}s)\")\n",
    "            else:\n",
    "                print(f\"  -> Could not parse JSON ({result['inference_time']:.1f}s)\")\n",
    "        else:\n",
    "            print(f\"  -> No JSON in response ({result['inference_time']:.1f}s)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  -> Error: {e}\")\n",
    "\n",
    "doc.close()\n",
    "\n",
    "# Deduplicate by test name (keep most complete)\n",
    "tests_by_name = {}\n",
    "for t in all_tests:\n",
    "    key = t.get('name', '').lower().strip()\n",
    "    if not key:\n",
    "        continue\n",
    "    score = sum(1 for f in ['value', 'unit', 'reference_range'] if t.get(f))\n",
    "    if key not in tests_by_name or score > sum(1 for f in ['value', 'unit', 'reference_range'] if tests_by_name[key].get(f)):\n",
    "        tests_by_name[key] = t\n",
    "\n",
    "unique_tests = list(tests_by_name.values())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"TOTAL: {len(unique_tests)} unique test results from {len(doc)} pages ({total_time:.1f}s total)\")\n",
    "print(\"=\" * 60)\n",
    "for t in unique_tests:\n",
    "    flag = f\" [{t.get('abnormal_flag')}]\" if t.get('abnormal_flag') else \"\"\n",
    "    print(f\"  {t.get('name')}: {t.get('value')} {t.get('unit', '')}{flag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client closed. All tests completed!\n"
     ]
    }
   ],
   "source": [
    "# Close the client session\n",
    "await client.close()\n",
    "\n",
    "print(\"Client closed. All tests completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/Users/yaokouadio/Projects/STARTUP/universal-medical-ingestion-engine/models/medgemma-4b-it-Q4_K_M.gguf'. Use `repo_type` argument if needed.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHFValidationError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/STARTUP/universal-medical-ingestion-engine/.venv/lib/python3.11/site-packages/transformers/utils/hub.py:479\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    478\u001b[39m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/STARTUP/universal-medical-ingestion-engine/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mrepo_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfrom_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mto_id\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m arg_name == \u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/STARTUP/universal-medical-ingestion-engine/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:154\u001b[39m, in \u001b[36mvalidate_repo_id\u001b[39m\u001b[34m(repo_id)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m repo_id.count(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    155\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must be in the form \u001b[39m\u001b[33m'\u001b[39m\u001b[33mrepo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mnamespace/repo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Use `repo_type` argument if needed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m     )\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX.match(repo_id):\n",
      "\u001b[31mHFValidationError\u001b[39m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/Users/yaokouadio/Projects/STARTUP/universal-medical-ingestion-engine/models/medgemma-4b-it-Q4_K_M.gguf'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mHFValidationError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m pipe = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage-text-to-text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/Users/yaokouadio/Projects/STARTUP/universal-medical-ingestion-engine/models/medgemma-4b-it-Q4_K_M.gguf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Image attribution: Stillwaterising, CC0, via Wikimedia Commons\u001b[39;00m\n\u001b[32m     14\u001b[39m image_url = \u001b[33m\"\u001b[39m\u001b[33mhttps://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/STARTUP/universal-medical-ingestion-engine/.venv/lib/python3.11/site-packages/transformers/pipelines/__init__.py:883\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m    879\u001b[39m     pretrained_model_name_or_path = model\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig) \u001b[38;5;129;01mand\u001b[39;00m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    882\u001b[39m     \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m883\u001b[39m     resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcache_dir\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    892\u001b[39m     hub_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/STARTUP/universal-medical-ingestion-engine/.venv/lib/python3.11/site-packages/transformers/utils/hub.py:322\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    265\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    266\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    267\u001b[39m     **kwargs,\n\u001b[32m    268\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    269\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    271\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/STARTUP/universal-medical-ingestion-engine/.venv/lib/python3.11/site-packages/transformers/utils/hub.py:531\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    525\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPermissionError at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m when downloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    526\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCheck cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    527\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m2) a previous download was canceled and the lock file needs manual removal.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    528\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    530\u001b[39m \u001b[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m resolved_files = \u001b[43m[\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_get_cache_file_to_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfull_filenames\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resolved_files):\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved_files\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/STARTUP/universal-medical-ingestion-engine/.venv/lib/python3.11/site-packages/transformers/utils/hub.py:532\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    525\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPermissionError at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m when downloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    526\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCheck cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    527\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m2) a previous download was canceled and the lock file needs manual removal.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    528\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    530\u001b[39m \u001b[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001b[39;00m\n\u001b[32m    531\u001b[39m resolved_files = [\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m     \u001b[43m_get_cache_file_to_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[32m    534\u001b[39m ]\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resolved_files):\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved_files\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/STARTUP/universal-medical-ingestion-engine/.venv/lib/python3.11/site-packages/transformers/utils/hub.py:143\u001b[39m, in \u001b[36m_get_cache_file_to_return\u001b[39m\u001b[34m(path_or_repo_id, full_filename, cache_dir, revision, repo_type)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_cache_file_to_return\u001b[39m(\n\u001b[32m    136\u001b[39m     path_or_repo_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    137\u001b[39m     full_filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    141\u001b[39m ):\n\u001b[32m    142\u001b[39m     \u001b[38;5;66;03m# We try to see if we have a cached version (not up to date):\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     resolved_file = \u001b[43mtry_to_load_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m resolved_file != _CACHED_NO_EXIST:\n\u001b[32m    147\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/STARTUP/universal-medical-ingestion-engine/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mzip\u001b[39m(signature.parameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[32m    103\u001b[39m     kwargs.items(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[32m    104\u001b[39m ):\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mrepo_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfrom_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mto_id\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m arg_name == \u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    109\u001b[39m         has_token = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/STARTUP/universal-medical-ingestion-engine/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:154\u001b[39m, in \u001b[36mvalidate_repo_id\u001b[39m\u001b[34m(repo_id)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRepo id must be a string, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(repo_id)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m repo_id.count(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    155\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must be in the form \u001b[39m\u001b[33m'\u001b[39m\u001b[33mrepo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mnamespace/repo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Use `repo_type` argument if needed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m     )\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX.match(repo_id):\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    161\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must use alphanumeric chars, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m The name cannot start or end with \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and the maximum length is 96:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    163\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m     )\n",
      "\u001b[31mHFValidationError\u001b[39m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/Users/yaokouadio/Projects/STARTUP/universal-medical-ingestion-engine/models/medgemma-4b-it-Q4_K_M.gguf'. Use `repo_type` argument if needed."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"image-text-to-text\",\n",
    "    model=\"/Users/yaokouadio/Projects/STARTUP/universal-medical-ingestion-engine/models/medgemma-4b-it-Q4_K_M.gguf\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "# Image attribution: Stillwaterising, CC0, via Wikimedia Commons\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png\"\n",
    "image = Image.open(requests.get(image_url, headers={\"User-Agent\": \"example\"}, stream=True).raw)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are an expert radiologist.\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Describe this X-ray\"},\n",
    "            {\"type\": \"image\", \"image\": image}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "output = pipe(text=messages, max_new_tokens=200)\n",
    "print(output[0][\"generated_text\"][-1][\"content\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
